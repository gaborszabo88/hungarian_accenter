{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.models import Input\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from progress_bar import log_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "PAD = 0\n",
    "ACCENT_TYPES = 4\n",
    "START = '^'  # start character\n",
    "END = '$'  # end character\n",
    "HUN = 'aábcdeéfghiíjklmnoóöőpqrstuúüűvwxyz'\n",
    "HUN += HUN.upper()\n",
    "\n",
    "CH_TO_ID = {c: i+1 for i,c in enumerate(START+END+HUN)}\n",
    "\n",
    "FILTER = {\n",
    "    'a': 'á',\n",
    "    'e': 'é',\n",
    "    'i': 'í',\n",
    "    'o': 'ó',\n",
    "    'u': 'ü',\n",
    "    'á': 'a',\n",
    "    'é': 'e',\n",
    "    'í': 'i',\n",
    "    'ó': 'o',\n",
    "    'ö': 'o',\n",
    "    'ő': 'o',\n",
    "    'ú': 'u',\n",
    "    'ü': 'u',\n",
    "    'ű': 'u',\n",
    "}\n",
    "\n",
    "LABELS = {\n",
    "    'a': ('a', 0),\n",
    "    'e': ('e', 0),\n",
    "    'i': ('i', 0),\n",
    "    'o': ('o', 0),\n",
    "    'u': ('u', 0),\n",
    "    'á': ('a', 1),\n",
    "    'é': ('e', 1),\n",
    "    'í': ('i', 1),\n",
    "    'ó': ('o', 1),\n",
    "    'ö': ('o', 2),\n",
    "    'ő': ('o', 3),\n",
    "    'ú': ('u', 1),\n",
    "    'ü': ('u', 2),\n",
    "    'ű': ('u', 3),\n",
    "}\n",
    "\n",
    "class DataProcessor():\n",
    "    \n",
    "    def __init__(self, file_list, window_size = 7):\n",
    "        self._window_size = 0\n",
    "        self._fill = 0\n",
    "        \n",
    "        self.set_window_size(window_size)\n",
    "        \n",
    "        self.file_list = file_list\n",
    "        \n",
    "        self.n = ACCENT_TYPES\n",
    "        self.max_len = 30\n",
    "        self.true_len = 0\n",
    "        \n",
    "        self.ch_to_id = CH_TO_ID\n",
    "        self.id_to_ch = {v:k for k,v in CH_TO_ID.items()}\n",
    "        self.words = set()\n",
    "        self.windows = {}\n",
    "        self.chars = defaultdict(int)\n",
    "        \n",
    "        self.get_unique_words()\n",
    "    \n",
    "    def set_window_size(self, x):\n",
    "        if x%2 != 1:\n",
    "            raise ValueError('Window size shall be an odd number!')\n",
    "        self._window_size = x\n",
    "        self._fill = int(x/2)\n",
    "    \n",
    "    def get_unique_words(self):\n",
    "        words = self.process_files()\n",
    "        for i in words:\n",
    "            self.words.add(i)\n",
    "    \n",
    "    def get_windows(self):\n",
    "        self.windows = {}\n",
    "        for w in self.words:\n",
    "            p = START*self._fill + w + END*self._fill\n",
    "            for i,c in enumerate(p):\n",
    "                if c in LABELS.keys():\n",
    "                    self.windows[p[i-self._fill:i+1+self._fill]] = c\n",
    "    \n",
    "    def filter_chars(self):\n",
    "        chars = defaultdict(int)\n",
    "        windows = {}\n",
    "        for key,c in self.windows.items():\n",
    "            if chars.get(c, 0) <= chars.get(FILTER[c], 0):\n",
    "                windows[key] = c\n",
    "                chars[c] += 1\n",
    "        self.windows = windows\n",
    "    \n",
    "    def get_id(self, c):\n",
    "        return self.ch_to_id.get(c, PAD)\n",
    "    \n",
    "    def get_char(self, idx, c = None):\n",
    "        return self.id_to_ch.get(idx, c)\n",
    "    \n",
    "    def process_files(self):\n",
    "        for file in log_progress(self.file_list, every=1):\n",
    "            with open(file, 'r') as f:\n",
    "                for line in f:\n",
    "                    if not line.startswith('#') and \\\n",
    "                       not line.startswith('\\n') and \\\n",
    "                       len(line.split()[0]) <= self.max_len:\n",
    "                        x = line.split()[0]\n",
    "                        if len(x) > self.true_len:\n",
    "                            self.true_len = len(x)\n",
    "                        yield line.split()[0]\n",
    "    \n",
    "    def decode(self, word):\n",
    "        out = []\n",
    "        for i in word:\n",
    "            out.append(self.get_id(i))\n",
    "        return np.array(out)\n",
    "    \n",
    "    def encode(self, idxs, orig = None):\n",
    "        if orig is None:\n",
    "            orig = '&'*len(idxs)\n",
    "        \n",
    "        out = []\n",
    "        for i,idx in enumerate(idxs):\n",
    "            out.append(self.get_char(idx, orig[i]))\n",
    "        return np.array(out)\n",
    "    \n",
    "    def remove_accent(self, word):\n",
    "        out = []\n",
    "        for c in word:\n",
    "            out.append(LABELS.get(c, [c, None])[0])\n",
    "        return out\n",
    "\n",
    "    def serve_data(self):\n",
    "        self.get_windows()\n",
    "        self.filter_chars()\n",
    "        \n",
    "        x_in = []\n",
    "        y_out = []\n",
    "        self.chars = defaultdict(int)\n",
    "        for key,c in self.windows.items():\n",
    "            self.chars[c] += 1\n",
    "            x_in.append(self.decode(self.remove_accent(list(key))))\n",
    "            y_out.append(np.array([LABELS[c][1]]))\n",
    "        \n",
    "        return np.array(x_in), np.array(y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NR_OF_FILES = None\n",
    "TRAIN_DATA_DIRS = [\n",
    "                   'comments_20131001-20131201.nlp',\n",
    "                   'comments_20131201-20140519.nlp',\n",
    "                   'comments_20140519-20140921.nlp',\n",
    "                  ]\n",
    "\n",
    "input_files = [os.path.join(dir,file) for dir in TRAIN_DATA_DIRS for file in os.listdir(dir)]\n",
    "provider = DataProcessor(input_files[:NR_OF_FILES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "provider.set_window_size(5)\n",
    "\n",
    "X, Y = provider.serve_data()\n",
    "Y = [to_categorical(i, num_classes=provider.n) for i in Y]\n",
    "X, X_te, Y, Y_te = train_test_split(X, Y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=provider.n, output_dim=128))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True, recurrent_dropout=0.1)))\n",
    "model.add(TimeDistributed(Dense(provider.n, activation=\"softmax\")))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss',\n",
    "                   min_delta=0,\n",
    "                   patience=3,\n",
    "                   verbose=0, mode='auto')\n",
    "\n",
    "history = model.fit(X, np.array(Y),\n",
    "                    batch_size=256,\n",
    "                    epochs=50,\n",
    "                    validation_split=0.1,\n",
    "                    verbose=1,\n",
    "                    callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_te, np.array(Y_te))\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "model.save('saprse_{}.h5'.format(str(int(time.time()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor():\n",
    "    def __init__(self, model, provider):\n",
    "        self.model = model\n",
    "        self.provider = provider\n",
    "    \n",
    "    def accent(self, w):\n",
    "        prediction = self.model.predict(provider.decode(list(a))).argmax(axis=2)\n",
    "        output = prediction.reshape(prediction.shape[0])\n",
    "        return ''.join(provider.encode(output, w))\n",
    "\n",
    "m = Predictor(model, provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m.accent('elnok')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
