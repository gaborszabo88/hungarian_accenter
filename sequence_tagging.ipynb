{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.models import Input\n",
    "\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from progress_bar import log_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def sparse_cross_entropy(y_true, y_pred):\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true,\n",
    "                                                          logits=y_pred)\n",
    "    loss_mean = tf.reduce_mean(loss)\n",
    "    return loss_mean\n",
    "\n",
    "decoder_target = tf.placeholder(dtype='int32', shape=(None, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "HUN = 'aábcdeéfghiíjklmnoóöőpqrstuúüűvwxyz'\n",
    "HUN += HUN.upper()\n",
    "CH_TO_ID = {c: i+1 for i,c in enumerate(HUN)}\n",
    "\n",
    "LABELS = {\n",
    "    'á': 'a',\n",
    "    'é': 'e',\n",
    "    'í': 'i',\n",
    "    'ó': 'o',\n",
    "    'ö': 'o',\n",
    "    'ő': 'o',\n",
    "    'ú': 'u',\n",
    "    'ü': 'u',\n",
    "    'ű': 'u',\n",
    "}\n",
    "\n",
    "class DataProcessor():\n",
    "    \n",
    "    def __init__(self, file_list):\n",
    "        self.file_list = file_list\n",
    "        \n",
    "        self.n = len(CH_TO_ID.keys())+1  # Zero is kept for padding and non-alphabetic characters\n",
    "        self.max_len = 30\n",
    "        self.true_len = 0\n",
    "        \n",
    "        self.ch_to_id = CH_TO_ID\n",
    "        self.id_to_ch = {v:k for k,v in CH_TO_ID.items()}\n",
    "        self.words = set()\n",
    "        \n",
    "        self.get_unique_words()\n",
    "    \n",
    "    def get_unique_words(self):\n",
    "        words = self.process_files()\n",
    "        for i in words:\n",
    "            self.words.add(i)\n",
    "    \n",
    "    def get_id(self, c):\n",
    "        return self.ch_to_id.get(c, PAD)\n",
    "    \n",
    "    def get_char(self, idx, c = None):\n",
    "        return self.id_to_ch.get(idx, c)\n",
    "    \n",
    "    def process_files(self):\n",
    "        for file in log_progress(self.file_list, every=1):\n",
    "            with open(file, 'r') as f:\n",
    "                for line in f:\n",
    "                    if not line.startswith('#') and \\\n",
    "                       not line.startswith('\\n') and \\\n",
    "                       len(line.split()[0]) <= self.max_len:\n",
    "                        x = line.split()[0]\n",
    "                        if len(x) > self.true_len:\n",
    "                            self.true_len = len(x)\n",
    "                        yield line.split()[0]\n",
    "    \n",
    "    def decode(self, word):\n",
    "        out = []\n",
    "        for i in word:\n",
    "            out.append(self.get_id(i))\n",
    "        return np.array(out)\n",
    "    \n",
    "    def encode(self, idxs, orig = None):\n",
    "        if orig is None:\n",
    "            orig = '$'*len(idxs)\n",
    "        \n",
    "        out = []\n",
    "        for i,idx in enumerate(idxs):\n",
    "            out.append(self.get_char(idx, orig[i]))\n",
    "        return np.array(out)\n",
    "    \n",
    "    def remove_accent(self, word):\n",
    "        out = word.copy()\n",
    "        for i,c in enumerate(word):\n",
    "            if c in LABELS.keys():\n",
    "                out[i] = LABELS[c][0]\n",
    "        return out\n",
    "\n",
    "    def serve_data(self):\n",
    "        x_in = []\n",
    "        y_out = []\n",
    "        for i in self.words:\n",
    "            x_in.append(self.decode(self.remove_accent(list(i))))\n",
    "            y_out.append(self.decode(i))\n",
    "        \n",
    "        return np.array(x_in), np.array(y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NR_OF_FILES = None\n",
    "TRAIN_DATA_DIRS = [\n",
    "                   'comments_20131001-20131201.nlp',\n",
    "                   'comments_20131201-20140519.nlp',\n",
    "                   'comments_20140519-20140921.nlp',\n",
    "                  ]\n",
    "\n",
    "input_files = [os.path.join(dir,file) for dir in TRAIN_DATA_DIRS for file in os.listdir(dir)]\n",
    "provider = DataProcessor(input_files[:NR_OF_FILES])\n",
    "\n",
    "X, Y = provider.serve_data()\n",
    "X = pad_sequences(maxlen=provider.true_len, sequences=X, padding=\"post\", value=PAD)\n",
    "Y = pad_sequences(maxlen=provider.true_len, sequences=Y, padding=\"post\", value=PAD)\n",
    "X, X_te, Y, Y_te = train_test_split(X, Y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=provider.n, output_dim=128))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True, recurrent_dropout=0.1)))\n",
    "model.add(TimeDistributed(Dense(provider.n, activation='linear')))\n",
    "\n",
    "model.compile(loss=sparse_cross_entropy,\n",
    "              optimizer='rmsprop',\n",
    "              target_tensors=[decoder_target])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss',\n",
    "                   min_delta=0,\n",
    "                   patience=3,\n",
    "                   verbose=0, mode='auto')\n",
    "\n",
    "history = model.fit(x_in, np.array(y_out),\n",
    "                    batch_size=512,\n",
    "                    epochs=50,\n",
    "                    validation_split=0.1,\n",
    "                    verbose=1,\n",
    "                    callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_te, np.array(Y_te))\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "model.save('saprse_{}.h5'.format(str(int(time.time()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Predictor():\n",
    "    def __init__(self, model, provider):\n",
    "        self.model = model\n",
    "        self.provider = provider\n",
    "    \n",
    "    def accent(self, w):\n",
    "        prediction = self.model.predict(provider.decode(list(a))).argmax(axis=2)\n",
    "        output = prediction.reshape(prediction.shape[0])\n",
    "        return ''.join(provider.encode(output, w))\n",
    "\n",
    "m = Predictor(model, provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.accent('elnok')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
